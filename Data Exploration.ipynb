{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import glob\n",
    "# from lxml import html\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "files = glob.glob('scotus/*json')\n",
    "\n",
    "            \n",
    "attributes_list = ['absolute_url', 'author', \n",
    "                   'author_str', 'cluster', \n",
    "                   'date_created','date_modified', \n",
    "                   'download_url', 'extracted_by_ocr',\n",
    "                   'html','html_columbia', \n",
    "                   'html_lawbox','html_with_citations', \n",
    "                   'id','joined_by', 'local_path', \n",
    "                   'opinions_cited','page_count','per_curiam','plain_text','resource_uri', 'sha1', 'type']\n",
    "\n",
    "\n",
    "attributes = {'absolute_url': 'unknown','author': 'unknown','author_str': 'unknown','cluster': 'unknown','date_created': 'unknown','date_modified': 'unknown','download_url': 'unknown','extracted_by_ocr': 'unknown','html': 'unknown','html_columbia': 'unknown','html_lawbox': 'unknown','html_with_citations': 'unknown','id': 'unknown','joined_by': 'unknown','local_path': 'unknown','opinions_cited': 'unknown','page_count': 'unknown','per_curiam': 'unknown','plain_text': 'unknown','resource_uri': 'unknown','sha1': 'unknown','type': 'unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f627a7e9c3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEmptyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/My3.6/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/My3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/My3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cases = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.loads(f.read(), object_hook=EmptyDict)\n",
    "        cases.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.page_count.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.per_curiam.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.joined_by.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plain_text = df.plain_text.apply(lambda x: x.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Omnibus Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some missing data (case names, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_of_decision = re.search(r\"Decided ([A-Z][a-z]+ [0-9]+, [0-9]{4})\", df.html_with_citations.iloc[0]).group(0)\n",
    "print(date_of_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_of_argument = re.search(r\"Argued ([A-Z][a-z]+ [0-9]+, [0-9]{4})\", df.html_with_citations.iloc[0]).group(0)\n",
    "print(date_of_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = re.search(r\"((?<!\\\\opinion\\\\\\d)([\\w-]+v(.)?[\\w-]+)(?!\\\\))\", df.local_path.iloc[0]).group(0).replace('_', ' ')\n",
    "print(short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(re.search(r\"Appendix [A-Z].*\", df.html_with_citations.iloc[0], re.S).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN OUT HEADERS, APPENDICES AND TABLES\n",
    "\n",
    "appendix_and_tables = re.search(r\"Appendix [A-Z] to(.)+?(Table \\d)\", df.html_with_citations.iloc[0], re.S).group()\n",
    "\n",
    "re_appx = re.compile(r\"Appendix [A-Z] to ,opinion of.*|APPENDIXES TO OPINION.*\", re.S)\n",
    "#us_const = r\"U(U\\. S\\.) Const\\.,? (((a|A)rt\\.?|(a|A)mend\\.?|(p|P)mbl\\.?|(p|P)reamble)( ?[XVI]+))?((, (s|S|&sect;|&#167) ([0-9]+)) ?(, cl\\. ([0-9]+)\\.?)?)\"\n",
    "amendments = re.compile(r\"((first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentienth|twenty(-)?first|twenty(-)?second|twenty(-)?third|twenty(-)?fourth|twenty(-)?fifth|twenty(-)?sixth|twenty(-)?seventh))+( amendment)( ?[XVI]+)\", re.I)\n",
    "\n",
    "\n",
    "def compile_regexp():\n",
    "    f_supp = r\"([0-9]+ (F\\. Supp\\. 2d\\.|F\\. ?Supp\\.) [0-9]+)\"\n",
    "#    us_const = r\"(U\\. S\\.) Const\\.,? (((a|A)rt\\.?|(a|A)mend\\.?|(p|P)mbl\\.?|(p|P)reamble)( ?[XVI]+))?((, (s|S|&sect;|&#167) ([0-9]+)) ?(, cl\\. ([0-9]+)\\.?)?)\"\n",
    "def remove_appendices(string):\n",
    "    regex = re.compile(r\"Appendix [A-Z] to ,opinion of.*|APPENDIXES TO OPINION.*\", re.S)\n",
    "    replacement = ''\n",
    "    string = re.sub(regex, '', string)\n",
    "    return string\n",
    "def remove_syllabus_and_headers(string):\n",
    "        string = re.sub(r'.+?(?=Opinion of)', '', string, count=1) # removes syllabus\n",
    "        string = re.sub(r\"/\\.x\", \"\", string)\n",
    "        string = re.sub(r'NOTICE.*?to press\\.', '', string, count=1) # removes slip op notice\n",
    "        string = re.sub(r\"SUPREME COURT OF THE UNITED STATES[_\\s]+No\\. \\d+–\\d+[_\\s]+\", \"\", string) # remove header\n",
    "        string = re.sub(r\"(\\b\\d+)?([\\s]*)?((Cite as:)? \\d{2,3} U\\. S\\. (\\d|_){4} \\(\\d{4}\\))[\\s]+\\d+[\\s]+(Opinion of [A-Z]+, (C\\.)? J\\.)?(, )?(concurring|dissenting)?(in part)?(and)?(concurring|dissenting)?(in part)?(in judgment)?[\\s]+?\", \"\", string) # partial header\n",
    "        string = re.sub(r\"(\\b\\d+)?([\\s]*)?((Cite as:)? \\d{2,3} U\\. S\\. (\\d|_){4} \\(\\d{4}\\))[\\s]+\\d+[\\s]+(Opinion of [A-Z]+, (C\\.)? J\\.)?[\\s]+?[A-Z\\.,\\s]+(, )?(concurring|dissenting)?(in part)?(and)?(concurring|dissenting)?(in part)?[\\s]+?(in judgment)?\", \"\", string)\n",
    "        string = re.sub(r\"\\b[\\d\\s]+([A-Z'\\s]+ v\\. [A-Z'’\\s]+)[\\s]+(Syllabus)?[\\s]+\", \"\", string) # inline citations\n",
    "        #string = re.sub(r\"\\s+Cite as: ?\\d+ U\\. ?S\\. ?[_\\d]+ \\(\\d{4}\\)?\\s+[\\d]+?\\s+?\", '', string, re.S) # removes citation headers\n",
    "        #string = re.sub(r\"[\\d]\\s.*?Opinion of.*?, (C\\. )?J\\.\", '', string, re.S) # Removes inline citations\n",
    "        #string = re.sub(r\"(?<=SUPREME COURT OF THE UNITED STATES)(.*?, APPELLANT(S)?) v\\. .*?(?=APPEAL FROM)\", '', string)\n",
    "        #string = re.sub(r\"SUPREME COURT OF THE UNITED STATES.*?\\d{4}\\]\", '', string, re.S)\n",
    "        return string\n",
    "        #[\\s]+?[A-Z\\.,\\s]+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and more data cleaning\n",
    "\n",
    "* BeautifulSoup\n",
    "* Citations\n",
    "* Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def court_soup(opinion):\n",
    "    soup = BeautifulSoup(opinion)\n",
    "    opinion = soup.text\n",
    "    opinion = re.sub(r'(\\n)+', '', opinion)\n",
    "    opinion = re.sub(r'(\\n)', ' ', opinion)\n",
    "    return opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove HTML tags from HTML with citations column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(df.iloc[64029].html_with_citations)\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['html_with_citations'].apply(lambda x: court_soup(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that aren't helping (because they're null, have low/no variance, are not germane to this project, or are available elsewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['date_created','author_str','date_created',\n",
    "           'date_modified','download_url','extracted_by_ocr','html_columbia',\n",
    "           'html_lawbox','local_path','opinions_cited','joined_by','resource_uri','sha1','type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle df\n",
    "# import pickle\n",
    "# with open(\"scotus_df.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(df, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get citation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cites = glob.glob('data/scotus_clusters/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = []\n",
    "for cite in cites:\n",
    "    with open(cite, 'r') as f:\n",
    "        data = json.loads(f.read(), object_hook=EmptyDict)\n",
    "        citations.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citedf = pd.DataFrame(citations)\n",
    "# pickle df\n",
    "# with open(\"cites_df.pickle\", \"wb\") as d:\n",
    "#    pickle.dump(citedf, d)\n",
    "# d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citedf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cluster.iloc[0].str.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citedf.id.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! For resuming work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scotus_df.pickle\", \"rb\") as f:\n",
    "    sc = pickle.load(f)\n",
    "with open(\"cites_df.pickle\", \"rb\") as c:\n",
    "    cd = pickle.load(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many cases are there per amendment of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = df['clean_text'].str.findall(r\"(first amend(\\.|ment)?|1st amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? I\\b|amend(\\.|ment) I\\b)\", re.I)\n",
    "    #r\"(first amend(\\.|ment)|1st amend(\\.|ment)|U(\\.)? ?S(\\.)? const(.)?amend(\\.|ment) I)\", re.I|re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1444\n",
    "first = first.apply(lambda x: len(x) > 0)\n",
    "has_first = df[first].copy()\n",
    "has_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1839\n",
    "fifth = df['clean_text'].str.findall(r\"(fifth amend(\\.|ment)?|5th amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? V\\b|amend(\\.|ment) V\\b)\", re.I|re.S)\n",
    "fifth = fifth.apply(lambda x: len(x) > 0)\n",
    "has_fifth = df[fifth].copy()\n",
    "has_fifth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 855\n",
    "sixth = df['clean_text'].str.findall(r\"(sixth amend(\\.|ment)?|6th amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? VI\\b|amend(\\.|ment) VI\\b)\", re.I|re.S)\n",
    "sixth = sixth.apply(lambda x: len(x) > 0)\n",
    "has_sixth = df[sixth].copy()\n",
    "has_sixth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 949\n",
    "fourth = df['clean_text'].str.findall(r\"(fourth amend(\\.|ment)?|4th amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? IV\\b|amend(\\.|ment) IV\\b)\", re.I|re.S)\n",
    "fourth = fourth.apply(lambda x: len(x) > 0)\n",
    "has_fourth = df[fourth].copy()\n",
    "has_fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 163\n",
    "second = df['clean_text'].str.findall(r\"(\\bsecond amend(\\.|ment)?|2nd amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? II\\b|amend(\\.|ment) II\\b)\", re.I|re.S)\n",
    "second = second.apply(lambda x: len(x) > 0)\n",
    "has_second = df[second].copy()\n",
    "has_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4579\n",
    "fourteenth = df['clean_text'].str.findall(r\"(fourteenth amend(\\.|ment)?|14th amend(\\.|ment)?|(U(\\.)? ?S(\\.)?)? const(.)? ?amend(\\.|ment)? XIV\\b|amend(\\.|ment) XIV\\b)\", re.I|re.S)\n",
    "fourteenth = fourteenth.apply(lambda x: len(x) > 0)\n",
    "has_fourteenth = df[fourteenth].copy()\n",
    "has_fourteenth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataframe for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for the target amendment (starting with: 14th Amendment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4579 cases \n",
    "has_fourteenth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create key to merge citation information with cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_keys = has_fourteenth.cluster.str.extractall(r'(?<=\\/)(\\d+)(?=\\/)')\n",
    "cluster_keys = cluster_keys.unstack()\n",
    "cluster_keys.columns = ['cluster_key']\n",
    "\n",
    "has_fourteenth['cluster_key'] = cluster_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citedf.id\n",
    "has_fourteenth['cluster_key'] = has_fourteenth['cluster_key'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge citations with cases\n",
    "cases_df = pd.merge(has_fourteenth, citedf,\n",
    "    how='left',\n",
    "    left_on='cluster_key',\n",
    "    right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4579 entries, 0 to 4578\n",
      "Data columns (total 55 columns):\n",
      "absolute_url_x               4579 non-null object\n",
      "author                       3322 non-null object\n",
      "cluster                      4579 non-null object\n",
      "html                         4528 non-null object\n",
      "html_with_citations          4579 non-null object\n",
      "id_x                         4579 non-null int64\n",
      "page_count                   215 non-null float64\n",
      "per_curiam                   4579 non-null bool\n",
      "plain_text                   4579 non-null object\n",
      "clean_text                   4579 non-null object\n",
      "cluster_key                  4579 non-null int64\n",
      "absolute_url_y               4578 non-null object\n",
      "attorneys                    4578 non-null object\n",
      "blocked                      4578 non-null object\n",
      "case_name                    4578 non-null object\n",
      "case_name_full               4578 non-null object\n",
      "case_name_short              4578 non-null object\n",
      "citation_count               4578 non-null float64\n",
      "citations                    4578 non-null object\n",
      "date_blocked                 0 non-null object\n",
      "date_created                 4578 non-null object\n",
      "date_filed                   4578 non-null object\n",
      "date_filed_is_approximate    4578 non-null object\n",
      "date_modified                4578 non-null object\n",
      "docket                       4578 non-null object\n",
      "federal_cite_one             4578 non-null object\n",
      "federal_cite_three           4578 non-null object\n",
      "federal_cite_two             4578 non-null object\n",
      "id_y                         4578 non-null float64\n",
      "judges                       4578 non-null object\n",
      "lexis_cite                   4578 non-null object\n",
      "nature_of_suit               4578 non-null object\n",
      "neutral_cite                 4578 non-null object\n",
      "non_participating_judges     4578 non-null object\n",
      "panel                        4578 non-null object\n",
      "posture                      4578 non-null object\n",
      "precedential_status          4578 non-null object\n",
      "procedural_history           4578 non-null object\n",
      "resource_uri                 4578 non-null object\n",
      "scdb_decision_direction      4106 non-null float64\n",
      "scdb_id                      4578 non-null object\n",
      "scdb_votes_majority          4106 non-null float64\n",
      "scdb_votes_minority          4106 non-null float64\n",
      "scotus_early_cite            4578 non-null object\n",
      "slug                         4578 non-null object\n",
      "source                       4578 non-null object\n",
      "specialty_cite_one           4578 non-null object\n",
      "state_cite_one               4578 non-null object\n",
      "state_cite_regional          4578 non-null object\n",
      "state_cite_three             4578 non-null object\n",
      "state_cite_two               4578 non-null object\n",
      "sub_opinions                 4578 non-null object\n",
      "syllabus                     4578 non-null object\n",
      "westlaw_cite                 4578 non-null object\n",
      "year                         4578 non-null float64\n",
      "dtypes: bool(1), float64(7), int64(2), object(45)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "cases_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_df.columns\n",
    "# cases_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_2 = ['absolute_url_x',\n",
    "'cluster',\n",
    "'html_with_citations',\n",
    "'html',\n",
    "'plain_text',\n",
    "'cluster_key',\n",
    "'absolute_url_y',\n",
    "'attorneys',\n",
    "'blocked',\n",
    "'case_name_full',\n",
    "'case_name_short',             \n",
    "'date_blocked',\n",
    "'date_created',\n",
    "'date_filed_is_approximate',\n",
    "'date_modified',\n",
    "'federal_cite_one',\n",
    "'federal_cite_three',\n",
    "'federal_cite_two', \n",
    "'id_y',\n",
    "'neutral_cite',     \n",
    "'scotus_early_cite',\n",
    "'slug',\n",
    "'specialty_cite_one',                                                            \n",
    "'state_cite_one',                                                                \n",
    "'state_cite_regional',                                                          \n",
    "'state_cite_three',                                                              \n",
    "'state_cite_two',\n",
    "'page_count',\n",
    "'procedural_history',\n",
    "'syllabus',\n",
    "'westlaw_cite']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "cases_df.drop(to_drop_2, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are they really duplicates? Masterpiece isn't really...\n",
    "dupes = cases_df[cases_df.duplicated(subset='case_name')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dupes.iloc[0]['clean_text']\n",
    "dupes.loc[4383]['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to extract the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df[\"year\"] = cases_df[\"date_filed\"].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle\n",
    "with open(\"target_df_2.pickle\", \"wb\") as t:\n",
    "    pickle.dump(cases_df, t)\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_df.to_csv(\"target_df_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "from nltk.stem.wordnet import wordnet, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"target_df\", \"rb\") as readfile:\n",
    "#   rcases_df = pickle.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcases_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcases_df.to_csv(\"target_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, parse, POS tag, lemmatize, NER, stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv('target_df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4579 entries, 0 to 4578\n",
      "Data columns (total 24 columns):\n",
      "author                      3322 non-null object\n",
      "id_x                        4579 non-null int64\n",
      "per_curiam                  4579 non-null bool\n",
      "clean_text                  4579 non-null object\n",
      "case_name                   4578 non-null object\n",
      "citation_count              4578 non-null float64\n",
      "citations                   4578 non-null object\n",
      "date_filed                  4578 non-null object\n",
      "docket                      4578 non-null object\n",
      "judges                      3827 non-null object\n",
      "lexis_cite                  4107 non-null object\n",
      "nature_of_suit              0 non-null float64\n",
      "non_participating_judges    4578 non-null object\n",
      "panel                       4578 non-null object\n",
      "posture                     0 non-null float64\n",
      "precedential_status         4578 non-null object\n",
      "resource_uri                4578 non-null object\n",
      "scdb_decision_direction     4106 non-null float64\n",
      "scdb_id                     4106 non-null object\n",
      "scdb_votes_majority         4106 non-null float64\n",
      "scdb_votes_minority         4106 non-null float64\n",
      "source                      4578 non-null object\n",
      "sub_opinions                4578 non-null object\n",
      "year                        4578 non-null float64\n",
      "dtypes: bool(1), float64(7), int64(1), object(15)\n",
      "memory usage: 827.3+ KB\n"
     ]
    }
   ],
   "source": [
    "r.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import lexnlp.nlp.en.transforms.tokens\n",
    "import lexnlp\n",
    "import lexnlp.extract.en.courts\n",
    "from typing import Generator\n",
    "\n",
    "import regex as re\n",
    "from reporters_db import EDITIONS, REPORTERS\n",
    "\n",
    "__author__ = \"ContraxSuite, LLC; LexPredict, LLC\"\n",
    "__copyright__ = \"Copyright 2015-2019, ContraxSuite, LLC\"\n",
    "__license__ = \"https://github.com/LexPredict/lexpredict-lexnlp/blob/master/LICENSE\"\n",
    "__version__ = \"0.2.6\"\n",
    "__maintainer__ = \"LexPredict, LLC\"\n",
    "__email__ = \"support@contraxsuite.com\"\n",
    "\n",
    "CITATION_PTN = r\"\"\"\n",
    "(?:[\\s,:\\(]|^)\n",
    "(\n",
    "(\\d+)\\s+\n",
    "({reporters})\\s+\n",
    "(\\d+)\n",
    "(?:,\\s+(\\d+(?:\\-\\d+)?))?\n",
    "(?:\\s+\\((.+?)?(\\d{{4}})\\))?\n",
    ")\n",
    "(?:\\W|$)\n",
    "\"\"\".format(reporters='|'.join([re.escape(i) for i in EDITIONS]))\n",
    "CITATION_PTN_RE = re.compile(CITATION_PTN, re.IGNORECASE | re.MULTILINE | re.DOTALL | re.VERBOSE)\n",
    "\n",
    "\n",
    "def get_citations(text, return_source=False, as_dict=False) -> Generator:\n",
    "    \"\"\"\n",
    "    Get citations.\n",
    "    :param text:\n",
    "    :param return_source:\n",
    "    :param as_dict:\n",
    "    :return: tuple or dict\n",
    "    (volume, reporter, reporter_full_name, page, page2, court, year[, source text])\n",
    "    \"\"\"\n",
    "    #https://github.com/freelawproject/reporters-db/blob/master/reporters_db/data/reporters.json\n",
    "    for source_text, volume, reporter, page, page2, court, year\\\n",
    "            in CITATION_PTN_RE.findall(text):\n",
    "        try:\n",
    "            reporter_data = REPORTERS[EDITIONS[reporter]]\n",
    "            reporter_full_name = ''\n",
    "            if len(reporter_data) == 1:\n",
    "                reporter_full_name = reporter_data[0]['name']\n",
    "            elif year:\n",
    "                for period_data in reporter_data:\n",
    "                    if reporter in period_data['editions']:\n",
    "                        start = period_data['editions'][reporter]['start'].year\n",
    "                        end = period_data['editions'][reporter]['end']\n",
    "                        if (end and start <= int(year) <= end.year) or start <= int(year):\n",
    "                            reporter_full_name = period_data['name']\n",
    "            item = (int(volume),\n",
    "                    reporter,\n",
    "                    reporter_full_name,\n",
    "                    int(page),\n",
    "                    page2 or None,\n",
    "                    court.strip(', ') or None,\n",
    "                    int(year) if year.isdigit() else None)\n",
    "            if return_source:\n",
    "                item += (source_text.strip(),)\n",
    "            if as_dict:\n",
    "                keys = ['volume', 'reporter', 'reporter_full_name',\n",
    "                        'page', 'page2', 'court', 'year', 'citation_str']\n",
    "                item = {keys[n]: val for n, val in enumerate(item)}\n",
    "            yield item\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_citation_noise(text):\n",
    "    stops = []\n",
    "    for cite in get_citations(text, return_source=True, as_dict=True):\n",
    "        # if len(cite['citation_str']) < 70:\n",
    "            # text = text.replace(cite[\"citation_str\"], \" \")\n",
    "            #    cite['citation_str'] = cite['citation_str'].replace(str(cite['volume']) + cite['reporter'] + str(cite['page']), \" \")\n",
    "            # stops.append(cite[\"citation_str\"])        \n",
    "        buildcite = str(cite['volume']) + \" \" + cite['reporter'] + \" \" + str(cite['page'])\n",
    "        if cite['page2']:\n",
    "            buildcite += ', '+str(cite['page2'])\n",
    "        stops.append(buildcite)\n",
    "        \n",
    "    return stops\n",
    "        \n",
    "def replace_citations(text):\n",
    "    stops = stop_citation_noise(text)\n",
    "    for stop in stops:\n",
    "        text = text.replace(stop, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "from nltk.stem.wordnet import wordnet, WordNetLemmatizer\n",
    "\n",
    "default_lemmatizer = WordNetLemmatizer()\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = set(stopwords.words('english')) \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "# Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # it's a noun if it's not found\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "    return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "def lemmatize_text(text, lemmatizer=default_lemmatizer):\n",
    "    tokens = tokenize_text(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens])\n",
    "\n",
    "def stem_text(text, stemmer=default_stemmer):\n",
    "    tokens = tokenize_text(text)\n",
    "    return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(text, stop_words=default_stopwords):\n",
    "    tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "    # cleaning pipeline in this function: \n",
    "    # remove extra spaces, lowercase, remove stopwords, stem_or_lem\n",
    "    \n",
    "def clean_text(text, stem_or_lem = 'lem'):\n",
    "    text = replace_citations(text)\n",
    "    #text = text.strip(' ') # strip whitespaces\n",
    "    text = re.sub(r\"[\\d]+\", \" \", text)\n",
    "    text = text.lower() # lowercase\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    if stem_or_lem == 'stem':\n",
    "        text = stem_text(text) # stemming\n",
    "    elif stem_or_lem == 'lem':\n",
    "        text = lemmatize_text(text) # lemmatizing\n",
    "    else: # intentionally breaking the argument so neither occurs\n",
    "        pass \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['corpora'] = r['clean_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4579 entries, 0 to 4578\n",
      "Data columns (total 25 columns):\n",
      "author                      3322 non-null object\n",
      "id_x                        4579 non-null int64\n",
      "per_curiam                  4579 non-null bool\n",
      "clean_text                  4579 non-null object\n",
      "case_name                   4578 non-null object\n",
      "citation_count              4578 non-null float64\n",
      "citations                   4578 non-null object\n",
      "date_filed                  4578 non-null object\n",
      "docket                      4578 non-null object\n",
      "judges                      3827 non-null object\n",
      "lexis_cite                  4107 non-null object\n",
      "nature_of_suit              0 non-null float64\n",
      "non_participating_judges    4578 non-null object\n",
      "panel                       4578 non-null object\n",
      "posture                     0 non-null float64\n",
      "precedential_status         4578 non-null object\n",
      "resource_uri                4578 non-null object\n",
      "scdb_decision_direction     4106 non-null float64\n",
      "scdb_id                     4106 non-null object\n",
      "scdb_votes_majority         4106 non-null float64\n",
      "scdb_votes_minority         4106 non-null float64\n",
      "source                      4578 non-null object\n",
      "sub_opinions                4578 non-null object\n",
      "year                        4578 non-null float64\n",
      "corpora                     4579 non-null object\n",
      "dtypes: bool(1), float64(7), int64(1), object(16)\n",
      "memory usage: 863.1+ KB\n"
     ]
    }
   ],
   "source": [
    "r.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_csv(\"cleaned_corpora.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>id_x</th>\n",
       "      <th>per_curiam</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>case_name</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>citations</th>\n",
       "      <th>date_filed</th>\n",
       "      <th>docket</th>\n",
       "      <th>judges</th>\n",
       "      <th>...</th>\n",
       "      <th>precedential_status</th>\n",
       "      <th>resource_uri</th>\n",
       "      <th>scdb_decision_direction</th>\n",
       "      <th>scdb_id</th>\n",
       "      <th>scdb_votes_majority</th>\n",
       "      <th>scdb_votes_minority</th>\n",
       "      <th>source</th>\n",
       "      <th>sub_opinions</th>\n",
       "      <th>year</th>\n",
       "      <th>corpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, id_x, per_curiam, clean_text, case_name, citation_count, citations, date_filed, docket, judges, lexis_cite, nature_of_suit, non_participating_judges, panel, posture, precedential_status, resource_uri, scdb_decision_direction, scdb_id, scdb_votes_majority, scdb_votes_minority, source, sub_opinions, year, corpora]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "dask_dataframe = ddf.from_pandas(df, npartitions=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create overlapping year ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = 1780\n",
    "last_year  = 2018\n",
    "increment  = 20\n",
    "overlap    = increment // 2\n",
    "# ------------------------\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "# intended use: include left, exclude right\n",
    "# half-closed, half-open interval [a, b)\n",
    "def build_year_ranges(first, last, inc, over):\n",
    "    year_ranges = []\n",
    "    for n in range(first, last, over):\n",
    "        year_ranges.append((n, n + inc))\n",
    "    return year_ranges\n",
    "\n",
    "\n",
    "# warning: years must have the same index as data\n",
    "def put_data_under_year_ranges(data, years, year_ranges):\n",
    "\n",
    "    # assert len(data) == len(years), \\\n",
    "    # \"get_content_under_ranges: data and years do not match length\"\n",
    "\n",
    "    # build a dict with keys = year_ranges, with a list for each range\n",
    "    data_ranges = dict()\n",
    "    for y in year_ranges:\n",
    "        data_ranges[y] = []\n",
    "\n",
    "    # bin all the data by range - each row should fall in two bins, \n",
    "    # if ranges are cleanly overlapped\n",
    "\n",
    "    # if data is a list\n",
    "    for i in range(len(data)):\n",
    "        for y in year_ranges:\n",
    "            if y[0] <= years[i] and years[i] < y[1]:\n",
    "                data_ranges[y].append(data[i])\n",
    "                # this should happen twice for every entry except \n",
    "                # the very oldest and the very newest\n",
    "\n",
    "    # pandas df\n",
    "\n",
    "    return data_ranges\n",
    "\n",
    "# ------------\n",
    "\n",
    "def run_year_range_build(): # main\n",
    "    \n",
    "    cases = []\n",
    "    years = []\n",
    "    corpora = []\n",
    "                  \n",
    "    for i in range(500):\n",
    "        years.append(first_year, last_year)\n",
    "        i = 0, len(corpora)-1\n",
    "        j = 0, len(corpora)-1\n",
    "        cases.append(corpora[i] + ' ' + corpora[j])\n",
    "        \n",
    "    # and bin them\n",
    "    bins = build_year_ranges(first_year, last_year, increment, overlap)\n",
    "    binned_data = put_data_under_year_ranges(cases, years, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.pickle\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def wm_to_df(wm, feat_names):\n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "\n",
    "def make_ngrams(docs):\n",
    "    custom_vec = CountVectorizer(ngram_range=(2,3))\n",
    "    corpora = []\n",
    "    for doc in tqdm.tqdm(nlp.pipe(docs)):\n",
    "        doc = [token.lemma_ for token in doc if (not token.is_stop and not token.is_digit and not token.is_punct)]\n",
    "        doc = \" \".join(doc)\n",
    "        corpora.append(doc)\n",
    "    wm = custom_vec.fit_transform(corpora)\n",
    "    tokens = custom_vec.get_feature_names()\n",
    "    df = wm_to_df(wm, tokens)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[100]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possible LSA workflow for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer to convert raw documents to TF/IDF matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "# Normalizes the vector (L2 norm of 1.0) to normalize \n",
    "# the effect of document length on tf-idf\n",
    "\n",
    "normalizer = Normalizer(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD\n",
    "# Project the tfidf vectors onto the first N principal components.\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=100,         // num dimensions\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10)\n",
    "\n",
    "lsa_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model),\n",
    "                            ('norm', normalizer)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_matrix = lsa_transformer.fit_transform(documents)\n",
    "\n",
    "print(\"Number of tf-idf features: {svd_matrix.get_shape()[1]}\")\n",
    "\n",
    "# Get the words that correspond to each of the features.\n",
    "feat_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for component_num in range(0, 100, 10):\n",
    "for component_num in range(0, 10):\n",
    "\n",
    "    comp = svd.components_[component_num]\n",
    "    \n",
    "    # Sort the weights in the first component and get indices\n",
    "    indices = numpy.argsort(comp).tolist()\n",
    "    \n",
    "    # Reverse order (largest weights first)\n",
    "    indices.reverse()\n",
    "    \n",
    "    # Get top 10 terms for component        \n",
    "    terms = [feat_names[weight_index] for weight_index in indices[0:10]]    \n",
    "    weights = [comp[weight_index] for weight_index in indices[0:10]]    \n",
    "   \n",
    "    # Display these terms and their weights as a horizontal bar graph.    \n",
    "    # The horizontal bar graph displays the first item on the bottom; reverse\n",
    "    # the order of the terms so the biggest one is on top.\n",
    "    terms.reverse()\n",
    "    weights.reverse()\n",
    "    positions = arange(10) + .5    # the bar centers on the y axis\n",
    "    \n",
    "    figure(component_num)\n",
    "    barh(positions, weights, align=\"center\")\n",
    "    yticks(positions, terms)\n",
    "    xlabel(\"Weight\")\n",
    "    title(\"Strongest terms for component {component_num}\")\n",
    "    grid(True)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Possible PLSA workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
